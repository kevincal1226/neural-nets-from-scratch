{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eAQFzY_V8WG6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/lukeyang01/nn-from-scratch/main/week2_data.csv')\n",
        "\n",
        "inputs = np.array(df.iloc[:, 0:2])\n",
        "labels = np.array(df.iloc[:, 2])"
      ],
      "metadata": {
        "id": "jqtpz4LKlx1w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x: int) -> float:\n",
        "  \"\"\"Sigmoid activation function.\"\"\"\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_back(x: int) -> float:\n",
        "  \"\"\"Derivative of sigmoid for backward calculation.\"\"\"\n",
        "  fwd = sigmoid(x)\n",
        "  return fwd * (1-fwd)"
      ],
      "metadata": {
        "id": "sYlAZMU4lKUg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  \"\"\"A Neural Network Class to Perform Basic Feedforward algorithm and training\"\"\"\n",
        "  def __init__(self, sizes: list):\n",
        "    \"\"\"Initialize a numpy array or a list of weights an array or list of weights depending on sizes\"\"\"\n",
        "    self.sizes = sizes\n",
        "    self.num_layers = len(sizes)\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    self.__init_params()\n",
        "    return\n",
        "\n",
        "  def __init_params(self):\n",
        "    # iteration over network layers\n",
        "      for i in range(1, self.num_layers):\n",
        "          # X inputs -> Y Ouputs\n",
        "          in_size = self.sizes[i-1]\n",
        "          out_size = self.sizes[i]\n",
        "\n",
        "          # weights.shape = (Y, X), biases.shape = (Y, 1)\n",
        "          # e.g. 2 -> 3: weights is (2,3), biases is (1,3)\n",
        "          self.weights.append(np.random.randn(in_size, out_size) * 0.1)\n",
        "          self.biases.append(np.random.randn(1, out_size) * 0.1)\n",
        "          # print(self.weights[-1].shape, self.biases[-1].shape)\n",
        "\n",
        "  def forward(self, x: np.ndarray):\n",
        "    \"\"\"\n",
        "      Perform feedforward algorithm on input vector for all layers\n",
        "\n",
        "      Input:    x: np.ndarray with shape (1, self.sizes[0])\n",
        "\n",
        "      Returns:  y: np.ndarray with shape (1, self.sizes[-1])\n",
        "    \"\"\"\n",
        "    # Reshape the input vector to match specs this way inputs can be passed in any shape.\n",
        "    x = x.reshape(1, self.sizes[0])\n",
        "    #print(f\"X {x}\")\n",
        "    #print(f\"Weight {self.weights[0]}\")\n",
        "    #print(f\"Bias {self.biases[0]}\")\n",
        "\n",
        "    # TODO: For each layer in network, perform feed forward algorithm\n",
        "    for i in range(self.num_layers - 1):\n",
        "      x = np.matmul(x, self.weights[i]) + self.biases[i]\n",
        "      if i != self.num_layers - 2:\n",
        "        for ix, iy in np.ndindex(x.shape):\n",
        "          x[ix][iy] = sigmoid(x[ix][iy])\n",
        "\n",
        "\n",
        "    # Hint: Use np.matmul instead of np.dot here (1x2 input) * (2x3 weights) -> (1x3 output)\n",
        "    # Hint: The zip function may be helpful but is non nessecary\n",
        "\n",
        "    return x\n",
        "\n",
        "  def backward(self, x, y):\n",
        "    \"\"\"Perform backpropagation using the input and expected output to get weight and vector deltas\"\"\"\n",
        "    return False\n",
        "\n",
        "  def train(self, X_train, y_train, epochs=1, lr=0.01, batch_size=1, verbose=True):\n",
        "    \"\"\"Using forward and backward functions, fit the model on an entire training step using gradient descent algorithm.\"\"\"\n",
        "    return False"
      ],
      "metadata": {
        "id": "kTAMUv0r8sIP"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  nn = MLP([2, 3, 2])\n",
        "  i = 0\n",
        "  for x in inputs:\n",
        "    val = nn.forward(x)\n",
        "    print(f\"Run {i} output: {val}\")\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "v_FfESHQ9yEt"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfXnBZHHse_p",
        "outputId": "4d3ae906-d2fe-4153-87f9-2d36c0f7844d"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 output: [[ 0.01948725 -0.05398238]]\n",
            "Run 1 output: [[ 0.01437326 -0.05933449]]\n",
            "Run 2 output: [[ 0.02971745 -0.04732893]]\n",
            "Run 3 output: [[ 0.03382136 -0.04289024]]\n",
            "Run 4 output: [[ 0.0109668  -0.06192188]]\n",
            "Run 5 output: [[ 0.02957319 -0.04724447]]\n",
            "Run 6 output: [[ 0.02198393 -0.05403979]]\n",
            "Run 7 output: [[ 0.02257874 -0.05388348]]\n",
            "Run 8 output: [[ 0.00146245 -0.07097139]]\n",
            "Run 9 output: [[ 0.02287627 -0.05096939]]\n",
            "Run 10 output: [[ 0.01732328 -0.05559785]]\n",
            "Run 11 output: [[ 0.01364992 -0.06079892]]\n",
            "Run 12 output: [[ 0.01714789 -0.05760858]]\n",
            "Run 13 output: [[ 0.0131969  -0.06036993]]\n",
            "Run 14 output: [[ 0.03259541 -0.04428286]]\n",
            "Run 15 output: [[ 0.00149892 -0.06964106]]\n",
            "Run 16 output: [[ 0.01580784 -0.05680949]]\n",
            "Run 17 output: [[ 0.01276349 -0.059622  ]]\n",
            "Run 18 output: [[ 0.01794065 -0.05516619]]\n",
            "Run 19 output: [[ 0.00887805 -0.06363945]]\n",
            "Run 20 output: [[ 0.01746851 -0.0574579 ]]\n",
            "Run 21 output: [[ 0.01479593 -0.05948379]]\n",
            "Run 22 output: [[ 0.02890809 -0.04656083]]\n",
            "Run 23 output: [[ 0.03170659 -0.04427103]]\n",
            "Run 24 output: [[ 0.00368185 -0.06854862]]\n",
            "Run 25 output: [[ 0.01730304 -0.05611754]]\n",
            "Run 26 output: [[ 0.01619761 -0.0590197 ]]\n",
            "Run 27 output: [[ 0.0047413  -0.06774006]]\n",
            "Run 28 output: [[ 0.01150069 -0.06326032]]\n",
            "Run 29 output: [[ 0.01747512 -0.0566138 ]]\n",
            "Run 30 output: [[ 0.01213167 -0.06201754]]\n",
            "Run 31 output: [[ 0.02852229 -0.04725468]]\n",
            "Run 32 output: [[ 0.00732537 -0.06494245]]\n",
            "Run 33 output: [[ 0.01948817 -0.05594437]]\n",
            "Run 34 output: [[ 0.01430817 -0.05969359]]\n",
            "Run 35 output: [[ 0.02663051 -0.05041791]]\n",
            "Run 36 output: [[ 0.00086931 -0.07038798]]\n",
            "Run 37 output: [[ 0.00724321 -0.06371986]]\n",
            "Run 38 output: [[ 0.02710732 -0.04739946]]\n",
            "Run 39 output: [[ 0.0174957  -0.05580927]]\n",
            "Run 40 output: [[ 0.00149484 -0.06947487]]\n",
            "Run 41 output: [[ 0.02122301 -0.05550524]]\n",
            "Run 42 output: [[ 0.01864659 -0.05538944]]\n",
            "Run 43 output: [[ 0.01778553 -0.05566736]]\n",
            "Run 44 output: [[ 0.03274367 -0.04392711]]\n",
            "Run 45 output: [[-0.00037664 -0.07128198]]\n",
            "Run 46 output: [[ 0.00227088 -0.06821175]]\n",
            "Run 47 output: [[ 0.01645497 -0.05501523]]\n",
            "Run 48 output: [[ 0.02730725 -0.04698937]]\n",
            "Run 49 output: [[ 0.05642933 -0.0164218 ]]\n",
            "Run 50 output: [[ 0.05928574 -0.01437734]]\n",
            "Run 51 output: [[ 0.0539702  -0.02028936]]\n",
            "Run 52 output: [[ 0.051172   -0.02107698]]\n",
            "Run 53 output: [[ 0.05697035 -0.01769867]]\n",
            "Run 54 output: [[ 0.05867732 -0.01582846]]\n",
            "Run 55 output: [[ 0.06026086 -0.01403834]]\n",
            "Run 56 output: [[ 0.05691102 -0.01659577]]\n",
            "Run 57 output: [[ 0.05586309 -0.01828768]]\n",
            "Run 58 output: [[ 0.05167484 -0.02077249]]\n",
            "Run 59 output: [[ 0.05632293 -0.01831475]]\n",
            "Run 60 output: [[ 0.05507034 -0.01847193]]\n",
            "Run 61 output: [[ 0.05031726 -0.02185707]]\n",
            "Run 62 output: [[ 0.06039019 -0.0137704 ]]\n",
            "Run 63 output: [[ 0.06150394 -0.01233442]]\n",
            "Run 64 output: [[ 0.05892117 -0.01527396]]\n",
            "Run 65 output: [[ 0.06064188 -0.01347266]]\n",
            "Run 66 output: [[ 0.06425565 -0.00964369]]\n",
            "Run 67 output: [[ 0.05964209 -0.01480276]]\n",
            "Run 68 output: [[ 0.05557643 -0.01776553]]\n",
            "Run 69 output: [[ 0.05842752 -0.01627994]]\n",
            "Run 70 output: [[ 0.05286915 -0.01964215]]\n",
            "Run 71 output: [[ 0.05683342 -0.01652102]]\n",
            "Run 72 output: [[ 0.05988507 -0.01509532]]\n",
            "Run 73 output: [[ 0.05783505 -0.01611237]]\n",
            "Run 74 output: [[ 0.06244037 -0.01135127]]\n",
            "Run 75 output: [[ 0.06008077 -0.01386091]]\n",
            "Run 76 output: [[ 0.05638271 -0.01723431]]\n",
            "Run 77 output: [[ 0.05658609 -0.01688288]]\n",
            "Run 78 output: [[ 0.05621895 -0.0169166 ]]\n",
            "Run 79 output: [[ 0.06335859 -0.01079907]]\n",
            "Run 80 output: [[ 0.06171231 -0.01312022]]\n",
            "Run 81 output: [[ 0.05899819 -0.01548951]]\n",
            "Run 82 output: [[ 0.05942344 -0.01551478]]\n",
            "Run 83 output: [[ 0.05602325 -0.01674296]]\n",
            "Run 84 output: [[ 0.05898862 -0.01478054]]\n",
            "Run 85 output: [[ 0.05182609 -0.02121036]]\n",
            "Run 86 output: [[ 0.05545666 -0.01851832]]\n",
            "Run 87 output: [[ 0.05270424 -0.0203724 ]]\n",
            "Run 88 output: [[ 0.06175707 -0.0132364 ]]\n",
            "Run 89 output: [[ 0.06210808 -0.01184952]]\n",
            "Run 90 output: [[ 0.06310247 -0.01050232]]\n",
            "Run 91 output: [[ 0.06302579 -0.01063223]]\n",
            "Run 92 output: [[ 0.0566484  -0.01914341]]\n",
            "Run 93 output: [[ 0.06068557 -0.01278523]]\n",
            "Run 94 output: [[ 0.06340543 -0.01064199]]\n",
            "Run 95 output: [[ 0.05724142 -0.01637427]]\n",
            "Run 96 output: [[ 0.05009607 -0.02340725]]\n",
            "Run 97 output: [[ 0.05578922 -0.01803996]]\n",
            "Run 98 output: [[ 0.0606284  -0.01360798]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UN-GskfktKDt"
      },
      "execution_count": 128,
      "outputs": []
    }
  ]
}