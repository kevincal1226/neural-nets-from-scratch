# -*- coding: utf-8 -*-
"""NN From Scratch Week1/2_starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/130o5O-B_jXvF1xgWHKaTZXxvh7lCZsh_
"""

import numpy as np
import pandas as pd
import math

# Week 1 Dataset
#df = pd.read_csv('https://raw.githubusercontent.com/lukeyang01/nn-from-scratch/main/week1_data.csv')

# TODO: First view the input data using head(), then store the top N inputs in a np array
#print(df.head())
# Hint: List[:x] can extract the first x elements of a python list
#listLength = 3
#inputs = df.head(listLength)
#print(inputs)

# Week 2 Dataset
df = pd.read_csv('https://raw.githubusercontent.com/lukeyang01/nn-from-scratch/main/week2_data.csv')
print(df.head())
inputs = np.array(df.iloc[:, 0:2])
labels = np.array(df.iloc[:, 2])

def sigmoid(x: int) -> float:
  """Sigmoid activation function."""
  return 1 / (1 + math.exp(-x))

def linear(x: int) -> float:
  """Linear activation function."""
  return x

def relu(x: int) -> float:
  """RELU activation function."""
  return max(x, 0)

def tanh(x: int) -> float:
  """Tanh activation function."""
  return math.tanh(x)

class Perceptron:
  """Neuron class object representing perceptron."""

  def __init__(self, num_inputs: int) -> None:
    """Init weights and other information."""

    self.num_inputs = num_inputs
    self.weights = np.random.rand(1, self.num_inputs)
    # Define random weights in shape (1, num_inputs)

  def feed_forward(self, inputs: np.array) -> float:
    """Forward propogation step."""
    val = np.dot(self.weights, inputs)
    return sigmoid(val[0])

  def train(self, inputs: list, labels: list,verbose = False, learning_rate = 0.25):
    """Training Step"""
    """
    k = 0
    for x, y in zip(inputs, labels):
      if verbose:
        print(f"Iteration: {k}, weights: {self.weights}")
      error = self.feed_forward(x) - y
      self.weights = self.weights - learning_rate * np.transpose(x * error)
      k += 1
    """
    for i in range(0, len(labels)):
      error = self.feed_forward(inputs[i]) - labels[i]
      dw = error * inputs[i]
      self.weights = self.weights - np.transpose(dw * learning_rate)
      if verbose:
        print(f"Iteration: {i}, weights: {self.weights}")

def main():
  """Test Perceptron class here."""

  p = Perceptron(inputs[0].shape[0])
  print(f"Perceptron weights before training: {p.weights}")
  p.train(inputs, labels, False)
  print(f"Perceptron weights after training: {p.weights}")
  #for i in range(0, len(labels)):
    #out = p.feed_forward(inputs[i])
    #print("Perceptron", i + 1, "output after forward propogation:", f"{out:.2f}")

main()

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def test_decision():
  """Test Perceptron decision boundary here."""
  # Create linearly separable dataset
  X = []
  y = []

  def line(x):
    """May adjust this to be any linear function with NO Y-INTERCEPT"""
    return 0.5*x

  # Iteration Range (will influence how many data points there are)
  for a in range(0, 1000):
    for b in range(0, 1000):
      # Division amount will determine the range [a, b]
      X.append(np.array([a/1000, b/1000]).reshape((2,1)))
      # Can adjust this to be any linear function (currently y = 0.5x). NO y - intercept
      if b > line(a):
        y.append(1)
      else:
        y.append(0)

  # Separate training data and testing data. Just returns a bunch of numpy arrays
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  p = Perceptron(2)
  p.train(X_train, y_train)

  x_m = [v1[0] for (v1, v2) in X_train[0:100]]
  y_m = [v2[0] for (v1, v2) in X_train[0:100]]

  col = []
  xs = np.linspace(0, 1, 100)
  ys = -p.weights[0][0]/p.weights[0][1] * xs

  for v1, v2 in zip(x_m, y_m):
    if v2 > line(v1):
      col.append("r")
    else:
      col.append("b")

  plt.plot(xs, ys)
  plt.scatter(x_m, y_m, c=col)
  plt.show()

test_decision()

